%%
%% This is LaTeX2e input.
%%

%% The following tells LaTeX that we are using the 
%% style file amsart.cls (That is the AMS article style
%%
\documentclass{amsart}

\usepackage{listings}
\usepackage{color}

\renewcommand\lstlistingname{Quelltext}

\lstset{
    language=Python,
    basicstyle=\small\sffamily,
    numbers=left,
    numberstyle=\tiny,
    frame=tb,
    tabsize=4,
    columns=fixed,
    showstringspaces=false,
    showtabs=false,
    keepspaces,
    commentstyle=\color{red},
    keywordstyle=\color{blue}
}

%% This has a default type size 10pt.  Other options are 11pt and 12pt
%% This are set by replacing the command above by
%% \documentclass[11pt]{amsart}
%%
%% or
%%
%% \documentclass[12pt]{amsart}
%%

%%
%% Some mathematical symbols are not included in the basic LaTeX
%% package.  Uncommenting the following makes more commands
%% available. 
%%

%\usepackage{amssymb}

%%
%% The following is commands are used for importing various types of
%% grapics.
%% 

%\usepackage{epsfig}  		% For postscript
%\usepackage{epic,eepic}       % For epic and eepic output from xfig

%%
%% The following is very useful in keeping track of labels while
%% writing.  The variant   \usepackage[notcite]{showkeys}
%% does not show the labels on the \cite commands.
%% 

%\usepackageshowkeys}


%%%%
%%%% The next few commands set up the theorem type environments.
%%%% Here they are set up to be numbered section.number, but this can
%%%% be changed.
%%%%

\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}


%%
%% If some other type is need, say conjectures, then it is constructed
%% by editing and uncommenting the following.
%%

%\newtheorem{conj}[thm]{Conjecture} 


%%% 
%%% The following gives definition type environments (which only differ
%%% from theorem type invironmants in the choices of fonts).  The
%%% numbering is still tied to the theorem counter.
%%% 

\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}
\newtheorem{example}[thm]{Example}

%%
%% Again more of these can be added by uncommenting and editing the
%% following. 
%%

%\newtheorem{note}[thm]{Note}


%%% 
%%% The following gives remark type environments (which only differ
%%% from theorem type invironmants in the choices of fonts).  The
%%% numbering is still tied to the theorem counter.
%%% 


\theoremstyle{remark}

\newtheorem{remark}[thm]{Remark}


%%%
%%% The following, if uncommented, numbers equations within sections.
%%% 

\numberwithin{equation}{section}


%%%
%%% The following show how to make definition (also called macros or
%%% abbreviations).  For example to use get a bold face R for use to
%%% name the real numbers the command is \mathbf{R}.  To save typing we
%%% can abbreviate as

\newcommand{\R}{\mathbf{R}}  % The real numbers.

%%
%% The comment after the defintion is not required, but if you are
%% working with someone they will likely thank you for explaining your
%% definition.  
%%
%% Now add you own definitions:
%%

%%%
%%% Mathematical operators (things like sin and cos which are used as
%%% functions and have slightly different spacing when typeset than
%%% variables are defined as follows:
%%%

\DeclareMathOperator{\dist}{dist} % The distance.



%%
%% This is the end of the preamble.
%% 


\begin{document}

%%
%% The title of the paper goes here.  Edit to your title.
%%

%%\title{Probabilistic Password Modeling: Applications of Machine Learning in Predicting Passwords}
\title{Probabilistic Password Modeling: Predicting Passwords with Machine Learning}

%%
%% Now edit the following to give your name and address:
%% 

%\author{Jay DeStories}
%\email{jaydestories@gmail.com}
%\urladdr{www.jwde.github.io} % Delete if not wanted.

%%
%% If there is another author uncomment and edit the following.
%%

%\author{Second Author}
%\address{Department of Mathematics, University of South Carolina,
%Columbia, SC 29208}
%\email{second@math.sc.edu}
%\urladdr{www.math.sc.edu/$\sim$second}

%%
%% If there are three of more authors they are added in the obvious
%% way. 
%%

%%%
%%% The following is for the abstract.  The abstract is optional and
%%% if not used just delete, or comment out, the following.
%%%

%\begin{abstract}
%Natural Language Processing Problem Set 2
%\end{abstract}

%%
%%  LaTeX will not make the title for the paper unless told to do so.
%%  This is done by uncommenting the following.
%%

\maketitle

\begin{center}
\small{JAY DESTORIES}\\
\small{MENTOR: ELIF YAMANGIL}\\[8ex]
\end{center}

%%
%% LaTeX can automatically make a table of contents.  This is done by
%% uncommenting the following:
%%

%\tableofcontents

%%
%%  To enter text is easy.  Just type it.  A blank line starts a new
%%  paragraph. 
%%



%%
%%  To put mathematics in a line it is put between dollor signs.  That
%%  is $(x+y)^2=x^2+2xy+y^2$
%%

%%
%%% Displayed mathematics is put between double dollar signs.  
%%


%%
%% A Theorem is stated by
%%

%\begin{thm} The square of any real number is non-negative.
%\end{thm}

%%
%% Its proof is set off by
%% 


%%
%% A new section is started as follows:
%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}\textbf{Abstract}\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Many systems use passwords as the primary means of authentication. As the length of a password grows, the search space of possible passwords grows exponentially. Despite this, people often fail to create unpredictable passwords. This paper will explore the problem of creating a probabilistic model for describing the distribution of passwords among the set of strings. This will help us gain insight into the relative strength of passwords as well as alternatives to existing methods of password candidate generation for password recovery tools like John the Ripper. This paper will consider methods from the field of natural language processing and evaluate their efficacy in modeling human-generated passwords.

\newpage

\tableofcontents

\newpage

\section{Introduction}
Since ancient times, passwords have been used as a means for authentication. Given the rise of personal computers and the internet, passwords are now commonly used by the general public for authentication with services like social networks or online banking. The purpose of a password, and authentication in general in the scope of computer systems, is to confirm an identity. For instance, when using an online banking service you provide a password so that you can perform actions on an account, but others cannot. This sense of security relies on a password being something that you can remember, but others cannot discover. The security of a system protected by a password is dependent on that password being difficult to guess. This is helped by the fact that an increase in the length of a password exponentially increases the number of possible passwords. Despite this, people often create unoriginal and easy to guess passwords. In a 2014 leak of approximately 5 million gmail account passwords, 47779 users had used the password "123456". In fact, there are so many instances of different people using the same passwords that one of the dominant ways to guess passwords in password recovery tools like John the Ripper is to simply guess every password in a list of leaked passwords. It is apparent that there is some underlying thought process in coming up with a password that leads to people creating similar or even identical passwords. This paper will attempt to find that structure in the form of a probabilistic model. It will consider some techniques from the field of natural language processing for inferring the structure of the language. If there is a good way to model the language of passwords, it could be used to generate new password guesses that don't already exist in a wordlist, improving the performance of password recovery tools when wordlists are exhausted. It could also evaluate the strength of a password in terms of the expected number of guesses to find it given a model.

\section{Classifying the Language of Passwords}
\subsection{Regular Languages}
\subsection{Context-Free Languages}
\subsection{Context-Sensitive and Recursively Enumerable Languages}

\section{Probability Modeling}
\subsection{N-Gram Language Model}
\subsection{Hidden Markov Model}
\subsection{Context-Free Grammar}

\section{Generating Passwords}
\subsection{Baseline}
\subsection{N-Gram Language Model}
\subsection{Hidden Markov Model}
\subsection{Context-Free Grammar}

\section{Applications}
\subsection{Password Strength}
\subsection{Password Recovery}

\section{Conclusion}

\newpage

\begin{thebibliography}{9}
\bibitem{latexcompanion}
Ghahramani, Z. (2001) An Introduction to Hidden Markov Models and Bayesian Networks.
\textit{International Journal of Pattern Recognition and Artificial Intelligence.}

\bibitem{latexcompanion}
Rosenfeld, Ronald. Two Decades of Statistical Language Modeling: Where do we go from here?
www.cs.cmu.edu/\textasciitilde{}roni/papers/survey-slm-IEEE-PROC-0004.pdf

\end{thebibliography}

\newpage

\section{Appendix}
\subsection{Language Model Evaluation}
\begin{lstlisting}
"""
    Test the performance of various password models
"""

import lmgenerator

# basic wordlist attack
def baselineGenerator(training_corpus):
    for pwd in training_corpus:
        yield pwd
    while True:
        yield ""

# See how many things in test_corpus the generator can guess with some number of
# tries
def testGenerator(gen, test_corpus, tries):
    found = 0
    test_set = set(test_corpus)
    guesses = set()
    for i in xrange(tries):
        guess = gen.next()
        if not guess in guesses:
            guesses.update([guess])
            if guess in test_set:
                found += 1
    return found

def testCorpora(training_corpus, test_corpus):
    print "First 5 training passwords: ", training_corpus[:5]
    print "First 5 test passwords: ", test_corpus[:5]

    tries = 100000
    baseline = testGenerator(baselineGenerator(training_corpus), test_corpus, tries)
    print "Baseline wordlist attack -- %d tries: %d." % (tries, baseline)
    bigramlmgen = lmgenerator.SimplePrunedBigramLMGenerator(training_corpus)
    bigramlm = testGenerator(bigramlmgen, test_corpus, tries)
    print "Bigram LM attack -- %d tries: %d." % (tries, bigramlm)
 

def main():
    print "################################################################"
    print "Training corpus: rockyou"
    print "Test corpus: gmail"
    print "################################################################"
    rockyou_nocount = open('corpora/rockyou_nocount', 'r')
    training_corpus = [pwd.rstrip() for pwd in rockyou_nocount]
    gmail_nocount = open('corpora/gmail_nocount', 'r')
    gmail_corpus = [pwd.rstrip() for pwd in gmail_nocount]
    test_corpus = gmail_corpus[:-5000]
    held_out_corpus = gmail_corpus[-5000:]
    testCorpora(training_corpus, test_corpus)


if __name__ == "__main__":
    main()
\end{lstlisting}
\subsection{N-Gram Language Model Implementation}
\begin{lstlisting}
from math import log, exp
import random

start_token = "<S>"
end_token = "</S>"

def Preprocess(corpus):
    return [[start_token] + [token for token in pwd] + [end_token] for pwd in corpus]

class BigramLM:
    def __init__(self):
        self.bigram_counts = {}
        self.unigram_counts = {}

    def Train(self, training_corpus):
        training_set = Preprocess(training_corpus)
        for pwd in training_set:
            for i in xrange(len(pwd) - 1):
                token = pwd[i]
                next_token = pwd[i + 1]
                if not token in self.unigram_counts:
                    self.unigram_counts[token] = 0
                if not token in self.bigram_counts:
                    self.bigram_counts[token] = {}
                if not next_token in self.bigram_counts[token]:
                    self.bigram_counts[token][next_token] = 0
                self.unigram_counts[token] += 1
                self.bigram_counts[token][next_token] += 1

    def GenerateSample(self):
        sample = [start_token]
        while not sample[-1] == end_token:
            selector = random.uniform(0, self.unigram_counts[sample[-1]])
            sum_bc = 0
            for bigram in self.bigram_counts[sample[-1]]:
                sum_bc += self.bigram_counts[sample[-1]][bigram]
                if sum_bc > selector:
                    sample.append(bigram)
                    break
        return ''.join(sample[1:-1])

    # gets the (unsmoothed) probability of a string given the bigramlm
#    def StringLogProbability(self, string):


def BigramLMGenerator(training_corpus):
    lm = BigramLM()
    lm.Train(training_corpus)
    while True:
        yield lm.GenerateSample()

def SimplePrunedBigramLMGenerator(training_corpus):
    tries = set()
    gen = BigramLMGenerator(training_corpus)
    while True:
        pwd = gen.next()
        if not pwd in tries:
            tries.update([pwd])
            yield pwd
\end{lstlisting}
\subsection{Hidden Markov Model Implementation}
\begin{lstlisting}
from math import log, exp, log1p
import random
from memoize import memoize

start_token = "<S>"
end_token = "</S>"
wildcard_token = "<*>"

# reduce floating point imprecision in adding probabilities in log space
def SumLogProbs(lps):
    # ln(e^lp1 + e^lp2) == ln(e^lp2 (e^(lp1 - lp2) + 1)) = ln(e^(lp1 - lp2) + 1) + lp2
    def adderhelper(lp1, lp2):
        return log1p(exp(lp1 - lp2)) + lp2 if lp2 > lp1 else log1p(exp(lp2 - lp1)) + lp1
    return reduce(adderhelper, lps)


def Preprocess(corpus):
    return [[start_token] + [token for token in pwd] + [end_token] for pwd in corpus]

# gets a count-length array of random probabilities summing to s
def RandomPartition(count, s):
    if count is 1:
        return [s]
    split_prob = (random.random() * .4 + .2) * s
    split_count = len(count) / 2
    return RandomPartition(split_count, split_prob) + \
           RandomPartition(count - split_count, s - split_prob)

# gets an array of log probabilities [p1, p2, ...] where e^p1 + e^p2 + ... = 1
def RandomLogProbs(count):
    total = 4000000000
    partition = RandomPartition(count, total)
    return [log(p) - log(total) for p in partition]


class BigramHMM:
    def __init__(self, vocabulary, state_count):
        self.o_vocabulary = set(vocabulary)
        self.states = range(state_count)
        self.start_probability = {state: prob for (state, prob) in zip(self.states, RandomLogProbs(state_count))}
        self.transition_probability = {state1: {state2: prob for (state2, prob) in (self.states, RandomLogProbs(state_count))} for state1 in self.states}
        self.end_probability = {state: prob for (state, prob) in zip(self.states, RandomLogProbs(state_count))}
        self.emission_probability = {state: {symbol: prob for (symbol, prob) in zip(vocabulary, RandomLogProbs(len(vocabulary)))} for state in self.states}

    @memoize
    def ForwardMatrix(pwd):
        bp = [{state: None for state in self.states} for c in pwd]

        # initialization
        bp[0] = {state: self.start_probability[state] + self.emission_probability[state][pwd[0]] for state in self.states}

        # recursion
        for i in xrange(1, len(pwd)):
            bp[i] = {state: SumLogProbs(map(lambda p: bp[i - 1][p] + self.transition_probability[p][state] + self.emission_probability[state][pwd[i]], bp[i - 1])) for state in self.states}

        return bp


    @memoize
    def BackwardMatrix(pwd):
        bp = [{state: None for state in self.states} for c in pwd]

        # initialization
        bp[len(pwd) - 1] = {state: self.end_probability[state] for state in self.states}

        # recursion
        for i in reversed(xrange(0, len(pwd) - 1)):
            bp[i] = {state: SumLogProbs(map(lambda n: bp[i + 1][n] + self.transition_probability[state][n] + self.emission_probability[n][pwd[i + 1]], bp[i + 1])) for state in self.states}

        return bp


    @memoize
    def ForwardProbability(step, state, pwd):
        matrix = self.ForwardMatrix(pwd)
        if state == wildcard_token:
            return SumLogProbs(matrix[step].values())
        return matrix[step][state]
        """
        if step is 0:
            return self.start_probability[end_state]

        bp = [{state: None for state in self.states} for c in pwd]

        # initialization
        bp[0] = {state: self.start_probability[state] + self.emission_probability[state][pwd[0]] for state in self.states}

        # recursion
        for i in xrange(1, step - 1):
            bp[i] = {state: sum(map(lambda p: bp[i - 1][p] + self.transition_probability[p][state] + self.emission_probability[state][pwd[i]], bp[i - 1])) for state in self.states}

        # termination
        if end_state == wildcard_token:
            return sum(map(lambda state: sum(map(lambda p: bp[step - 1][p] + self.transition_probability[p][state] + self.emission_probability[state][pwd[step]], bp[step - 1])), bp[step]))

        return sum(map(lambda p: bp[step - 1][p] + self.transition_probability[p][end_state] + self.emission_probability[end_state][pwd[step]], bp[step - 1]))
    """

    @memoize
    def BackwardProbability(step, state, pwd):
        matrix = self.BackwardMatrix(pwd)
        return matrix[step][state]
        """
        last_step = len(pwd) - 1
        if step == last_step:
            return self.end_probability[start_state]
        
        bp = [{state: None for state in self.states} for c in pwd]

        # initialization
        bp[last_step] = {state: self.end_probability[state] for state in self.states}

        # recursion
        for i in reversed(xrange(step + 1, last_step - 1)):
            bp[i] = {state: sum(map(lambda n: bp[i + 1][n] + self.transition_probability[state][n] + self.emission_probability[n][pwd[i + 1]], bp[i + 1])) for state in self.states}

        # termination
        return sum(map(lambda n: bp[step + 1][n] + self.transition_probability[start_state][n] + self.emission_probability[n][pwd[step + 1]], bp[step + 1]))
    """
        
    @memoize
    def TimeStateProbability(step, state, pwd):
        return self.ForwardProbability(step, state, pwd) + \
               self.BackwardProbability(step, state, pwd) - \
               self.ForwardProbability(len(pwd) - 1, wildcard_token, pwd)

    @memoize
    def StateTransitionProbability(step, state1, state2, pwd):
        return self.ForwardProbability(step, state1, pwd) + \
               self.BackwardProbability(step + 1, state2, pwd) + \
               self.transition_probability[state1][state2] + \
               self.emission_probability[state2][pwd[step + 1]] - \
               self.ForwardProbability(len(pwd) - 1, wildcard_token, pwd)

    def ForwardBackward():
        # for now assume convergence in constant number of iterations
        for i in xrange(10):
            # expectation 
            
            # maximization
        
            # reset memos
            self.ForwardMatrix.reset()
            self.BackwardMatrix.reset()
            self.ForwardProbability.reset()
            self.BackwardProbability.reset()
            self.TimeStateProbability.reset()
            self.StateTransitionProbability.reset()
\end{lstlisting}
\subsection{Context-Free Grammar Implementation}

\end{document}
