%%
%% This is LaTeX2e input.
%%

%% The following tells LaTeX that we are using the 
%% style file amsart.cls (That is the AMS article style
%%
\documentclass{amsart}

\usepackage{listings}
\usepackage{color}

\renewcommand\lstlistingname{Quelltext}

\lstset{
    language=Python,
    basicstyle=\small\sffamily,
    numbers=left,
    numberstyle=\tiny,
    frame=tb,
    tabsize=4,
    columns=fixed,
    showstringspaces=false,
    showtabs=false,
    keepspaces,
    commentstyle=\color{red},
    keywordstyle=\color{blue}
}

%% This has a default type size 10pt.  Other options are 11pt and 12pt
%% This are set by replacing the command above by
%% \documentclass[11pt]{amsart}
%%
%% or
%%
%% \documentclass[12pt]{amsart}
%%

%%
%% Some mathematical symbols are not included in the basic LaTeX
%% package.  Uncommenting the following makes more commands
%% available. 
%%

%\usepackage{amssymb}

%%
%% The following is commands are used for importing various types of
%% grapics.
%% 

%\usepackage{epsfig}  		% For postscript
%\usepackage{epic,eepic}       % For epic and eepic output from xfig

%%
%% The following is very useful in keeping track of labels while
%% writing.  The variant   \usepackage[notcite]{showkeys}
%% does not show the labels on the \cite commands.
%% 

%\usepackageshowkeys}


%%%%
%%%% The next few commands set up the theorem type environments.
%%%% Here they are set up to be numbered section.number, but this can
%%%% be changed.
%%%%

\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}


%%
%% If some other type is need, say conjectures, then it is constructed
%% by editing and uncommenting the following.
%%

%\newtheorem{conj}[thm]{Conjecture} 


%%% 
%%% The following gives definition type environments (which only differ
%%% from theorem type invironmants in the choices of fonts).  The
%%% numbering is still tied to the theorem counter.
%%% 

\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}
\newtheorem{example}[thm]{Example}

%%
%% Again more of these can be added by uncommenting and editing the
%% following. 
%%

%\newtheorem{note}[thm]{Note}


%%% 
%%% The following gives remark type environments (which only differ
%%% from theorem type invironmants in the choices of fonts).  The
%%% numbering is still tied to the theorem counter.
%%% 


\theoremstyle{remark}

\newtheorem{remark}[thm]{Remark}


%%%
%%% The following, if uncommented, numbers equations within sections.
%%% 

\numberwithin{equation}{section}


%%%
%%% The following show how to make definition (also called macros or
%%% abbreviations).  For example to use get a bold face R for use to
%%% name the real numbers the command is \mathbf{R}.  To save typing we
%%% can abbreviate as

\newcommand{\R}{\mathbf{R}}  % The real numbers.

%%
%% The comment after the defintion is not required, but if you are
%% working with someone they will likely thank you for explaining your
%% definition.  
%%
%% Now add you own definitions:
%%

%%%
%%% Mathematical operators (things like sin and cos which are used as
%%% functions and have slightly different spacing when typeset than
%%% variables are defined as follows:
%%%

\DeclareMathOperator{\dist}{dist} % The distance.



%%
%% This is the end of the preamble.
%% 


\begin{document}

%%
%% The title of the paper goes here.  Edit to your title.
%%

%%\title{Probabilistic Password Modeling: Applications of Machine Learning in Predicting Passwords}
\title{Probabilistic Password Modeling: Predicting Passwords with Machine Learning}

%%
%% Now edit the following to give your name and address:
%% 

%\author{Jay DeStories}
%\email{jaydestories@gmail.com}
%\urladdr{www.jwde.github.io} % Delete if not wanted.

%%
%% If there is another author uncomment and edit the following.
%%

%\author{Second Author}
%\address{Department of Mathematics, University of South Carolina,
%Columbia, SC 29208}
%\email{second@math.sc.edu}
%\urladdr{www.math.sc.edu/$\sim$second}

%%
%% If there are three of more authors they are added in the obvious
%% way. 
%%

%%%
%%% The following is for the abstract.  The abstract is optional and
%%% if not used just delete, or comment out, the following.
%%%

%\begin{abstract}
%Natural Language Processing Problem Set 2
%\end{abstract}

%%
%%  LaTeX will not make the title for the paper unless told to do so.
%%  This is done by uncommenting the following.
%%

\maketitle

\begin{center}
\small{JAY DESTORIES}\\
\small{MENTOR: ELIF YAMANGIL}\\[8ex]
\end{center}

%%
%% LaTeX can automatically make a table of contents.  This is done by
%% uncommenting the following:
%%

%\tableofcontents

%%
%%  To enter text is easy.  Just type it.  A blank line starts a new
%%  paragraph. 
%%



%%
%%  To put mathematics in a line it is put between dollor signs.  That
%%  is $(x+y)^2=x^2+2xy+y^2$
%%

%%
%%% Displayed mathematics is put between double dollar signs.  
%%


%%
%% A Theorem is stated by
%%

%\begin{thm} The square of any real number is non-negative.
%\end{thm}

%%
%% Its proof is set off by
%% 


%%
%% A new section is started as follows:
%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}\textbf{Abstract}\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Many systems use passwords as the primary means of authentication. As the length of a password grows, the search space of possible passwords grows exponentially. Despite this, people often fail to create unpredictable passwords. This paper will explore the problem of creating a probabilistic model for describing the distribution of passwords among the set of strings. This will help us gain insight into the relative strength of passwords as well as alternatives to existing methods of password candidate generation for password recovery tools like John the Ripper. This paper will consider methods from the field of natural language processing and evaluate their efficacy in modeling human-generated passwords.

\newpage

\tableofcontents

\newpage

\section{Introduction}
\section{Classifying the Language of Passwords}
\subsection{Regular Languages}
\subsection{Context-Free Languages}
\subsection{Context-Sensitive and Recursively Enumerable Languages}

\section{Probability Modeling}
\subsection{N-Gram Language Model}
\subsection{Hidden Markov Model}
\subsection{Context-Free Grammar}

\section{Generating Passwords}
\subsection{Baseline}
\subsection{N-Gram Language Model}
\subsection{Hidden Markov Model}
\subsection{Context-Free Grammar}

\section{Applications}
\subsection{Password Strength}
\subsection{Password Recovery}

\section{Conclusion}

\newpage

\begin{thebibliography}{9}
\bibitem{latexcompanion}
Ghahramani, Z. (2001) An Introduction to Hidden Markov Models and Bayesian Networks.
\textit{International Journal of Pattern Recognition and Artificial Intelligence.}

\bibitem{latexcompanion}
Rosenfeld, Ronald. Two Decades of Statistical Language Modeling: Where do we go from here?
www.cs.cmu.edu/\textasciitilde{}roni/papers/survey-slm-IEEE-PROC-0004.pdf

\end{thebibliography}

\newpage

\section{Appendix}
\subsection{Language Model Evaluation}
\begin{lstlisting}
"""
    Test the performance of various password models
"""

# basic wordlist attack
def baselineGenerator(training_corpus):
    for pwd in training_corpus:
        yield pwd
    while True:
        yield ""

# See how many things in test_corpus the generator can guess with some number of
# tries
def testGenerator(gen, test_corpus, tries):
    found = 0
    test_set = set(test_corpus)
    guesses = set()
    for i in xrange(tries):
        guess = gen.next()
        if not guess in guesses:
            guesses.update([guess])
            if guess in test_set:
                found += 1
    return found

def testCopora(training_corpus, test_corpus):
    print "First 5 training passwords: ", training_corpus[:5]
    print "First 5 test passwords: ", test_corpus[:5]

    tries = 1000000
    baseline = testGenerator(baselineGenerator(training_corpus), test_corpus, tries)
    print "Baseline wordlist attack -- %d tries: %d." % (tries, baseline)
 

def main():
    print "################################################################"
    print "Training corpus: rockyou"
    print "Test corpus: gmail"
    print "################################################################"
    rockyou_nocount = open('corpora/rockyou_nocount', 'r')
    training_corpus = [pwd.rstrip() for pwd in rockyou_nocount]
    gmail_nocount = open('corpora/gmail_nocount', 'r')
    gmail_corpus = [pwd.rstrip() for pwd in gmail_nocount]
    test_corpus = gmail_corpus[:-5000]
    held_out_corpus = gmail_corpus[-5000:]
    testCorpora(training_corpus, test_corpus)


if __name__ == "__main__":
    main()
\end{lstlisting}
\subsection{N-Gram Language Model Implementation}
\subsection{Hidden Markov Model Implementation}
\subsection{Context-Free Grammar Implementation}

\end{document}
